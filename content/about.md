---
title: "About"
type: "page"
---

This very infrequent blog is by me, <span style="color:#ff5a33"> **Nicola Branchini**</span>.  
I am PhD student in Statistics in the [School of Mathematics at the University of Edinburgh](https://www.maths.ed.ac.uk/school-of-mathematics/research/data-decisions/statistics/stats-people), advised by [Dr. Víctor Elvira](https://victorelvira.github.io/).

{{< figure src="profile_pic.jpeg" class="pull-right" >}}

I am interested broadly in Statistics and its applications to Machine Learning. Recently, I have started working on optimization methods within **Monte Carlo** (with particular focus on Sequential Monte Carlo and importance sampling), as well as Bayesian approaches to **causal optimization** and discovery.

Some topics I am exploring now are:
- Efficient adaptive **importance sampling** schemes for statistical decision making
- Methods of fusion of interventional and observational data for efficient **causal estimation**

### News

- I will be giving a talk at the [Causal Inference Interest Group (CIIG)](https://neildhir.github.io/ciig/) at the Alan Turing Institute.


## Publications
- [***Optimized Auxiliary Particle Filters: adapting mixture proposals via convex optimization***](https://proceedings.mlr.press/v161/branchini21a.html), Branchini, Nicola and Elvira, Víctor. To appear in *37th Conference on Uncertainty in Artificial Intelligence (UAI), Proceedings of Machine Learning Research, 2021*.

<details>
	<summary style="color:#ff5a33"> Details about paper</summary>
In this paper we wanted to improve on the Auxiliary Particle Filter (APF), which is thought for estimating the likelihood in sequential latent variable models with very informative observations. This algorithm however still has severe drawbacks; among some, the resampling weights are chosen independently, i.e. each particle chooses its own without "knowing" what the others are doing.

We devise a new way to optimize these resampling weights by viewing them as mixture weights of an importance sampling mixture proposal. It turns out that choosing mixture weights in order to minimize the resulting empirical variance of the importance weights leads to a convex optimization problem.

<img src="/assets/images/eq_oapf.svg" width="1000" height="300">
</details>

<br>
 [**A link to my resume**](https://resume.io/r/pgbpr3wNh)
<br>

### Talks & Posters

- Poster: *Optimized Auxiliary Particle Filters: adapting mixture proposals via convex optimization*, at ["Bayes at CIRM" Winter School](https://bayesatcirm.github.io/), Centre International de Rencontres Mathématiques, Marseille, October 2021
- Poster: *Optimized Auxiliary Particle Filters: adapting mixture proposals via convex optimization* at 37th Conference on Uncertainty in Artificial Intelligence (UAI), online, 2021.

### Awards
- [Feuer International Scholarship in Artificial Intelligence (full funding)](https://warwick.ac.uk/services/dc/schols_fund/scholarships_and_funding/feuer/scholarships/) (declined)
- [School of Mathematics Studentship, University of Edinburgh (full funding)](https://www.maths.ed.ac.uk/school-of-mathematics/studying-here/pgr/funding-opportunities)
- Dissertation Prize for the Artificial Intelligence MSc, *University of Edinburgh*
- Oustanding dissertation award (BSc Computer Science), *University of Warwick*

### Some background

Previously, I was a Research Assistant at the [Alan Turing Institute]( https://www.turing.ac.uk/), working within the [Warwick Machine Learning Group](https://wmlg.io/) and supervised by [Prof. Theo Damoulas](https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/damoulas). Previous to that, I was a Master's student in the [School of Informatics at the University of Edinburgh](https://www.ed.ac.uk/informatics) where I was supervised by [Dr. Víctor Elvira](https://victorelvira.github.io/) working on auxiliary particle filters.
As undergrad, I studied Computer Science at the [University of Warwick](https://warwick.ac.uk/fac/sci/dcs/), where I did my BSc dissertation on reproducing AlphaZero supervised by [Dr. Paolo Turrini](https://www.dcs.warwick.ac.uk/~pturrini/).


### Great reads
Totally worth having the physical version.
-  [Noise: A Flaw in Human Judgment, Daniel Kahneman, Olivier Sibony, Cass R. Sunstein](https://www.amazon.co.uk/Noise-Daniel-Kahneman/dp/0008309000)
- [The book of why, Judea Pearl & Dana McKenzie](https://www.amazon.co.uk/Book-Why-Science-Cause-Effect/dp/0141982411/ref=sr_1_1?dchild=1&keywords=the+book+of+why+judea&qid=1589542460&sr=8-1)
- [Meditations, Marcus Aurelius](https://www.amazon.co.uk/Meditations-Penguin-Classics-Marcus-Aurelius/dp/0140449337/ref=sr_1_1?crid=RRDPU6IPJQ5N&dchild=1&keywords=meditations+marcus+aurelius&qid=1589542523&sprefix=meditations+%2Caps%2C154&sr=8-1)
- [Sustainable energy - without the hot air, David McKay](http://www.withouthotair.com/)
- [The Art of Statistics: Learning from Data, David Spiegelhalter](https://www.amazon.co.uk/Learning-Data-Statistics-Pelican-Books/dp/0241258766/ref=tmm_pap_swatch_0?_encoding=UTF8&qid=&sr=)
- [Probably Approximately Correct, Leslie Valiant](https://www.amazon.co.uk/Probably-Approximately-Correct-Algorithms-Prospering/dp/0465060722/ref=sr_1_1?dchild=1&keywords=probably+approximately+correct&qid=1599657065&sr=8-1)


### Cool random stuff
- The [Machine Learning Summer School videos 2009 (!)](http://videolectures.net/mlss09uk_cambridge/)
- [Ferenc Huszár's blog](https://www.inference.vc/)
- [Michael Betancourt's blog](https://betanalpha.github.io/writing/?fbclid=IwAR32LpCi6bC6SwqGmsbfmo147GhKrfup7P4JY0_o2jiW6dT9BQ58arigx8M)
- [CMU blog on ML](https://blog.ml.cmu.edu/)
- [Terence Tao's blog](https://terrytao.wordpress.com/)
- [Troubling Trends in Machine Learning Scholarship](https://arxiv.org/pdf/1807.03341.pdf?source=post_page---------------------------)
